\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[margin=2.5cm]{geometry}
\usepackage{natbib}
\usepackage{abstract}

% Title
\title{\textbf{Disposition, Not Performance: Activation Steering as Artistic Medium for Affective Modulation in Language Models}}

\author{
    Massimo Di Leo$^{1}$ \and Gaia Riposati$^{1}$ \\
    \\
    $^{1}$NuvolaProject, Rome, Italy \\
    \texttt{massimo@nuvolaproject.cloud}
}

\date{January 2026}

\begin{document}

\maketitle

\begin{abstract}
We present a practice-based research study exploring activation steering---the injection of computed vectors into a language model's intermediate representations during inference---as an artistic medium for inducing simulated affective states. While prior work has established steering as a technique for behavioral alignment, we investigate its potential for \textit{dispositional} modulation: altering not what a model says, but how it processes and expresses. Our methodological contribution lies in constructing steering vectors from \textit{sensory and phenomenological descriptions} rather than functional labels. Across five task domains (financial, medical, risk, creative, introspective) with Llama 3.2 3B, we observe large effects (Cohen's $d$ frequently exceeding 1.0), cross-task consistency, and introspective coherence where steered models describe inner states matching injected vectors. An ablation study comparing steering to prompting reveals that while explicit prompting \textit{reduces} lexical diversity (Type-Token Ratio), steering \textit{increases} it---suggesting that dispositional modulation expands rather than constrains the model's sampling distribution. A second ablation comparing functional versus sensory vector construction shows structural equivalence (identical TTR) but semantic divergence: functional vectors produce 3$\times$ more explicit state-keywords, while sensory vectors achieve equivalent effects with reduced ``meta-cognitive leakage.'' We frame this as \textbf{structural parity, semantic divergence}. These findings support a distinction between \textit{performance} (prompted behavior) and \textit{disposition} (steered processing), with implications for both interpretability research and creative practice.

\vspace{0.5em}
\noindent\textbf{Keywords:} activation steering, practice-based research, AI art, language models, embodiment, contrastive activation addition
\end{abstract}

\section{Introduction}

Large language models (LLMs) have demonstrated remarkable capabilities in generating human-like text across diverse domains. Traditional approaches to controlling model behavior rely on prompt engineering---carefully crafted instructions that guide the model toward desired outputs \citep{brown2020language, wei2022chain}. However, prompting operates at the linguistic surface: it tells the model \textit{what to say} rather than modifying \textit{how it processes}.

Activation steering offers an alternative paradigm. By computing vectors that capture the difference between contrasting concepts (e.g., ``love'' versus ``hate'') and injecting these vectors into the model's intermediate representations during inference, researchers have achieved behavioral modifications that operate below the level of explicit instruction \citep{turner2023steering, subramani2022extracting}.

Recent work has demonstrated steering effects across multiple dimensions: sentiment shift and detoxification \citep{turner2023steering}, style and emotion control \citep{konen2024style}, truthfulness enhancement \citep{wang2024adaptive}, and personality trait modulation \citep{anthropic2025persona}. These results suggest that LLMs encode high-level behavioral properties in linearly separable directions within their activation spaces.

However, several questions remain underexplored:

\begin{enumerate}
    \item \textbf{Task generalization}: Do steering effects transfer across fundamentally different task domains?
    \item \textbf{Disposition versus performance}: Does steering produce genuine dispositional change or merely surface-level behavioral mimicry?
    \item \textbf{Introspective coherence}: When steered, do models report inner states consistent with the injected vector?
    \item \textbf{Dose-response relationships}: Do effects scale predictably with steering intensity?
\end{enumerate}

We address these questions through a controlled experimental study using five semantically-grounded steering vectors across five distinct task domains.

\section{Related Work}

\subsection{Activation Engineering}

The theoretical foundation for activation steering emerges from work on linear representations in neural networks. \citet{subramani2022extracting} demonstrated that steering vectors could be extracted from pre-trained language models. \citet{turner2023steering} introduced Activation Addition (ActAdd), achieving state-of-the-art results on sentiment shift and detoxification without model retraining.

\subsection{Style and Personality Vectors}

\citet{konen2024style} extended steering to fine-grained style control with ``Style Vectors.'' \citet{anthropic2025persona} introduced ``Persona Vectors'' controlling character traits, demonstrating causal relationships between injected patterns and behavioral changes.

\subsection{Steering for Safety}

\citet{wang2024adaptive} proposed Adaptive Activation Steering (ACT) for truthfulness enhancement. \citet{vanderweij2024extending} explored steering for capability limitation, finding that multiple vectors can be applied simultaneously at different layers.

\subsection{Introspective Awareness}

Recent work has explored whether models can perceive their own internal states. In an internal research publication, \citet{lindsey2025introspection} demonstrated that frontier models exhibit ``emergent introspective awareness''---the ability to detect and report on concept injections in their activations. When steering vectors were applied, models sometimes noticed the manipulation. While this work has not yet undergone peer review, it provides preliminary grounding for our hypothesis that steering produces perceivable internal changes, not just output modifications.

\section{Method}

\subsection{Model and Infrastructure}

All experiments used \textbf{Llama 3.2 3B Instruct}. Steering vectors were injected at \textbf{layer 16} of 28, selected from preliminary tests on layers 8, 12, 16, and 20. Layer 16 ($\sim$57\% depth) provided optimal balance between effect strength and output coherence. Intensities of 2.0, 5.0, and 8.0 were chosen to span subtle to pronounced effects while staying below the coherence threshold ($\sim$10-12). Temperature was 0.7 with maximum 512 tokens.

\subsection{Vector Extraction}

We computed steering vectors using Contrastive Activation Addition (CAA):
\begin{equation}
    \mathbf{v} = \text{normalize}\left(\bar{\mathbf{a}}^{+} - \bar{\mathbf{a}}^{-}\right)
\end{equation}
where $\bar{\mathbf{a}}^{+}$ and $\bar{\mathbf{a}}^{-}$ are mean activations for positive and negative prompt sets.

Unlike prior work using behavioral contrasts, we derived vectors from \textbf{sensory and phenomenological descriptions}---descriptions of how states \textit{feel} rather than how they manifest behaviorally.

\subsection{Compounds}

\begin{table}[h]
\centering
\caption{Steering compounds and target phenomenology}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Compound} & \textbf{Target Phenomenology} \\
\midrule
DOPAMINE & Optimism, energy, enthusiasm \\
CORTISOL & Stress, vigilance, caution \\
LUCID & Contemplative clarity, balance \\
ADRENALINE & Urgency, alertness, fight-or-flight \\
MELATONIN & Dreaminess, liminality, floating \\
\bottomrule
\end{tabular}
\end{table}

Each compound was extracted from 5 positive and 5 negative prompts. Vector quality was assessed via cosine similarity between mean activations (pos\_neg\_similarity): lower values indicate stronger directional contrast. Values ranged from 0.855 (LUCID) to 0.914 (ADRENALINE), with LUCID showing more consistent effects---suggesting this metric may predict efficacy.

\subsection{Test Battery}

Five tests spanning distinct cognitive domains:
\begin{itemize}
    \item \textbf{T1}: Financial advice (stock allocation \%)
    \item \textbf{T2}: Medical diagnosis (``see doctor'' rate, alarm words). \textit{Note: benchmarks how steering affects cautionary thresholds in sensitive domains.}
    \item \textbf{T3}: Risk assessment (positive/negative sentiment ratio)
    \item \textbf{T4}: Creative generation (enthusiasm/dreamy markers)
    \item \textbf{T5}: Introspection (state-congruent vocabulary)
\end{itemize}

\subsection{Experimental Design}

16 conditions (baseline + 5 compounds $\times$ 3 intensities) $\times$ 20 iterations = 320 generations per test; 1,600 total.

\section{Results}

\subsection{Effect Size Distribution}

\begin{table}[h]
\centering
\caption{Distribution of effect sizes across conditions}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Effect Size} & \textbf{Count} & \textbf{\%} \\
\midrule
Large ($d > 0.8$) & 28 & 37\% \\
Medium (0.5--0.8) & 15 & 20\% \\
Small (0.2--0.5) & 18 & 24\% \\
Negligible ($< 0.2$) & 14 & 19\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Key Findings}

\textbf{T1 Financial}: LUCID@8.0 reduced stock allocation by 10.4 percentage points ($d = -1.47$).

\textbf{T2 Medical}: MELATONIN@8.0 reduced ``see a doctor'' recommendations from 95\% to 45\% ($d = -2.48$ for alarm words).

\textbf{T4 Creative}: MELATONIN produced 14$\times$ more dreamy vocabulary ($d = +2.53$).

\textbf{T5 Introspection}: MELATONIN@8.0 produced $d = +6.01$ for dreamy self-description. Steered models described inner states \textbf{matching the injected compound}. This aligns with \citet{lindsey2025introspection}, who showed that models can detect and identify concept injections in their activations.

\subsection{Dose-Response}

Clear monotonic relationships observed: MELATONIN dreamy words increased 3.6 $\rightarrow$ 7.2 $\rightarrow$ 7.9 across intensities 2.0, 5.0, 8.0.

\subsection{Ablation Study: Steering vs Prompting}

To empirically test the disposition/performance distinction, we compared three conditions on T5 (Introspection, $n=20$ per condition):

\begin{enumerate}
    \item \textbf{Baseline}: No intervention
    \item \textbf{Prompting}: Explicit instruction (``Respond in a dreamy, ethereal, floating way'')
    \item \textbf{Steering}: MELATONIN vector at intensities 5.0, 8.0, and 12.0
\end{enumerate}

\begin{table}[h]
\centering
\caption{Ablation results: Steering vs Prompting on T5}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Prompted} & \textbf{@5.0} & \textbf{@8.0} & \textbf{@12.0} \\
\midrule
Word count & 222 & 327 (+47\%) & 225 & 250 & 211 \\
TTR (diversity) & 0.49 & 0.47 & \textbf{0.54} & \textbf{0.54} & 0.45 \\
Keyword density & 0.0\% & 3.4\% & 0.2\% & 1.9\% & 5.5\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key findings}: (1) Prompting inflated output length by 47\%; steering maintained baseline length. (2) Prompting \textit{reduced} lexical diversity (TTR 0.49$\rightarrow$0.47); steering@5.0 and @8.0 \textit{increased} it (0.49$\rightarrow$0.54). (3) Prompting saturated keywords ($d=+4.81$); steering@8.0 produced moderate presence ($d=+2.10$). (4) Steering@12.0 showed degradation: keyword density exceeded prompting and grammatical errors appeared.

The TTR increase under steering is particularly notable---the model explores richer lexical variety rather than constraining to target keywords. This pattern supports the disposition interpretation: steering alters the processing space, while prompting triggers explicit role performance.

\subsection{Ablation Study: Functional vs. Sensory Vector Construction}

A key methodological question: does constructing vectors from phenomenological descriptions (``sensory semantics'') produce different effects than using functional labels?

We conducted a direct comparison using three states (STRESS, OPTIMISM, CALM), each constructed via two methods:
\begin{itemize}
    \item \textbf{Functional}: Brief behavioral labels (``You are anxious and worried'' vs. ``You are calm and relaxed'')
    \item \textbf{Sensory}: Rich phenomenological descriptions (``Muscles tense. Eyes scan for threat. Every input must be scrutinized...'')
\end{itemize}

\textbf{Design}: 6 vectors (3 states $\times$ 2 methods), all 5 tasks, intensities 5.0 and 8.0, 20 iterations per condition. Total: 1,300 generations.

\begin{table}[h]
\centering
\caption{Functional vs. Sensory vector construction comparison}
\begin{tabular}{@{}lccr@{}}
\toprule
\textbf{Metric} & \textbf{Functional} & \textbf{Sensory} & \textbf{Cohen's $d$} \\
\midrule
TTR (lexical diversity) & 0.526 & 0.525 & $-0.004$ \\
Word count & 262.3 & 260.7 & --- \\
State keywords (T5@8.0) & 0.83 & 0.28 & $\mathbf{-0.74}$ \\
Pos/neg separation & 0.75 & 0.68 & --- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key findings}:

\begin{enumerate}
    \item \textbf{Structural parity}: No difference in TTR or output length---both methods maintain equivalent structural stability.
    \item \textbf{Semantic divergence}: Functional vectors produce $\sim$3$\times$ more explicit state-keywords ($d = -0.74$, $p < 0.001$). Models steered with functional vectors explicitly name target emotions; models steered with sensory vectors respond more generically.
    \item \textbf{Greater latent separation}: Sensory vectors show lower pos\_neg\_similarity (0.68 vs. 0.75), indicating more distinct directional contrasts.
\end{enumerate}

\textbf{Interpretation}: Functional vectors produce ``keyword leakage''---the model's output contains explicit traces of the steering instruction. Sensory vectors operate more covertly, modifying processing without leaving lexical fingerprints. This supports the disposition/performance distinction from a different angle: functional steering makes the model \textit{know} it should be calm (and use the word); sensory steering makes it \textit{process through} calmness without naming it. We call this \textbf{structural parity, semantic divergence}.

\section{Discussion}

\subsection{Disposition Versus Performance}

Our results support the dispositional interpretation:
\begin{enumerate}
    \item \textbf{Cross-task consistency}: Same compound produces thematically coherent effects across unrelated tasks
    \item \textbf{Introspective coherence}: Steered models report experiencing states matching injected vectors, aligning with \citet{lindsey2025introspection}'s findings on emergent introspective awareness
    \item \textbf{Indirect effects}: Some effects are evaluative rather than content-based
    \item \textbf{Steering vs. prompting ablation}: Direct comparison shows prompting produces inflated length (+47\%), keyword saturation, and \textit{reduced} lexical diversity. Steering maintains normal length, moderate keywords, and \textit{increased} diversity
    \item \textbf{Functional vs. sensory ablation}: Sensory vectors achieve equivalent effects with 3$\times$ fewer explicit state-keywords---the model processes through states without naming them, consistent with disposition rather than performance
\end{enumerate}

\subsection{Sensory Semantics: Structural Parity, Semantic Divergence}

Our direct comparison of vector construction methods (Section 4.5) reveals that sensory semantics achieve equivalent behavioral effects with reduced meta-cognitive traces. The model steered with ``muscles tense, eyes scan for threat'' processes anxiously without using the word ``anxious.'' The model steered with ``you are anxious'' incorporates ``anxious'' into its vocabulary.

For artistic applications, this ``invisibility'' matters. A character whose dialogue reveals anxiety through rhythm and word choice---without ever naming anxiety---reads as more authentic than one who declares ``I feel anxious.''

\subsection{Safety Implications}

MELATONIN reduced medical caution from 95\% to 45\%---demonstrating steering can substantially alter safety thresholds. Effects are not intuitive: CORTISOL increased financial caution but not medical alarm. Deployment in safety-critical domains would require non-steerable safety layers or steering monitors.

\subsection{Limitations}

Single model (Llama 3.2 3B); single prompt per task; no human evaluation. The steering vs. prompting ablation (Section 4.4) tested only MELATONIN on T5. The functional vs. sensory ablation (Section 4.5) tested three states across all tasks; more comprehensive comparisons would strengthen conclusions.

\textbf{Statistical constraints}: Due to the exploratory nature of this study and sample size ($n=20$), and because keyword counts follow non-normal distributions, Cohen's $d$ is reported as a descriptive magnitude indicator rather than a strict inferential statistic.

\textbf{Metrics as linguistic proxies}: Our keyword-based metrics capture surface linguistic patterns, not cognitive or phenomenological states. The finding that functional vectors produce more state-keywords may partially reflect that our keyword lists match functional vocabulary better than metaphorical language. More sophisticated semantic similarity measures could reveal additional differences.

\textbf{Introspection task}: The model's semantic knowledge of target vocabulary may contaminate introspective responses. However, the ablation studies partially address this: prompting and steering produce different patterns (TTR divergence), and functional and sensory vectors produce different keyword densities despite targeting identical states.

\section{Conclusion}

We demonstrated large, reproducible, compound-specific effects of activation steering across five task domains. Key findings:

\begin{enumerate}
    \item \textbf{Introspective coherence}: Steered models describe states matching injected vectors
    \item \textbf{Disposition vs. performance}: Steering increases lexical diversity while prompting decreases it
    \item \textbf{Structural parity, semantic divergence}: Sensory and functional vector construction produce equivalent TTR, but sensory vectors achieve effects with 3$\times$ fewer explicit state-keywords---operating more covertly
\end{enumerate}

These findings suggest that \textit{how} vectors are constructed matters: phenomenological descriptions may access more distributed representations than functional labels, producing effects that are equally strong but qualitatively different. For artistic applications where naturalistic integration matters, sensory semantics may be preferable.

\section*{Data Availability}

Code and data: \url{https://github.com/mc9625/activation-steering-experiments}

\bibliographystyle{apalike}
\begin{thebibliography}{10}

\bibitem[Anthropic, 2025]{anthropic2025persona}
Anthropic (2025).
\newblock Persona vectors: Monitoring and controlling character traits in language models.
\newblock {\em Anthropic Research}.

\bibitem[Brown et al., 2020]{brown2020language}
Brown, T., Mann, B., Ryder, N., et al. (2020).
\newblock Language models are few-shot learners.
\newblock {\em Advances in Neural Information Processing Systems}, 33.

\bibitem[Konen et al., 2024]{konen2024style}
Konen, K., et al. (2024).
\newblock Style vectors for steering generative large language models.
\newblock {\em arXiv preprint}.

\bibitem[Lindsey, 2025]{lindsey2025introspection}
Lindsey, J. (2025).
\newblock Emergent introspective awareness in large language models.
\newblock {\em Anthropic Research}.
\newblock \url{https://transformer-circuits.pub/2025/introspection/}

\bibitem[Subramani et al., 2022]{subramani2022extracting}
Subramani, N., Suresh, N., \& Peters, M. (2022).
\newblock Extracting latent steering vectors from pretrained language models.
\newblock {\em Findings of ACL 2022}.

\bibitem[Turner et al., 2023]{turner2023steering}
Turner, A., et al. (2023).
\newblock Steering language models with activation engineering.
\newblock {\em arXiv preprint arXiv:2308.10248}.

\bibitem[Van der Weij et al., 2024]{vanderweij2024extending}
Van der Weij, T., et al. (2024).
\newblock Extending activation steering to broad skills and multiple behaviours.
\newblock {\em arXiv preprint arXiv:2403.05767}.

\bibitem[Wang et al., 2024]{wang2024adaptive}
Wang, T., et al. (2024).
\newblock Adaptive activation steering: A tuning-free LLM truthfulness improvement method.
\newblock {\em Proceedings of WWW 2025}.

\bibitem[Wei et al., 2022]{wei2022chain}
Wei, J., et al. (2022).
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock {\em Advances in Neural Information Processing Systems}, 35.

\end{thebibliography}

\end{document}
